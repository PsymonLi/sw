# -*- mode: ruby -*-
# vi: set ft=ruby :

require 'fileutils'

cluster_ip_nodes = ""

basic = <<BASIC_SCRIPT
## setup basic environment for the VM
echo -n "$1" > /etc/hostname
hostname -F /etc/hostname
chown -R vagrant /import

rm -f /etc/machine-id
systemd-machine-id-setup

## Populate environment vars
echo 'export PATH=$PATH:/usr/local/go/bin:/opt/bin:/import/bin' >> /etc/profile.d/env.sh
echo 'export GOPATH=/import/' >> /etc/profile.d/env.sh
echo 'export PEN_NODES=$3' >> /etc/profile.d/env.sh
source /etc/profile.d/env.sh

# stop firewall service and disable selinux
systemctl stop firewalld
setenforce 0

# Start docker and swarm
systemctl enable docker && systemctl start docker
sudo chmod a+rw /var/run/docker.sock

BASIC_SCRIPT

VAGRANTFILE_API_VERSION = "2"
Vagrant.configure(VAGRANTFILE_API_VERSION) do |config|
  config.vm.box = "jainvipin/docker-centos73"
  # config.vm.box_version = "0.1"

  num_nodes = 1
  if ENV['PENS_NODES'] && ENV['PENS_NODES'] != "" then
      num_nodes = ENV['PENS_NODES'].to_i
  end
  base_ip = "192.168.50."
  if ENV['PENS_IP_PREFIX'] && ENV['PENS_IP_PREFIX'] != "" then
      base_ip = ENV['PENS_IP_PREFIX']
  end
  node_ips = num_nodes.times.collect { |n| base_ip + "#{n+11}" }
  cluster_ip_nodes = node_ips.join(",")

  config.ssh.insert_key = false
  node_names = num_nodes.times.collect { |n| "node#{n+1}" }

  num_nodes.times do |n|
    node_name = node_names[n]
    node_addr = node_ips[n]

    config.vm.define node_name do |node|
      # Control Interface
      node.vm.network :private_network, ip: node_addr
      # Data Interface
      node.vm.network :private_network, ip: "0.0.0.0", virtualbox__intnet: "data_net", auto_config: false

      node.vm.provider "virtualbox" do |v|
          v.cpus = 2
          v.memory = 3072
          v.linked_clone = true # use base image and clone from it. for multi-VM, saves space

          # this makes provisioning faster for yum-install type of stuff with caching
          if Vagrant.has_plugin?("vagrant-cachier")
              # ... vagrant-cachier configs ... makes provisioning faster by caching packages
              config.cache.scope = :machine
          end


          # enable 'virtio' on control nics to take benefit of builtin vlan tag
          # use intel e1000 nics on data NIC so that we can run DPDK on it
          v.customize ['modifyvm', :id, '--nictype1', 'virtio']
          v.customize ['modifyvm', :id, '--nictype2', 'virtio']
          v.customize ['modifyvm', :id, '--nictype3', '82545EM']

          v.customize ['modifyvm', :id, '--nicpromisc2', 'allow-all']
          v.customize ['modifyvm', :id, '--nicpromisc3', 'allow-all']
          v.customize ['modifyvm', :id, '--paravirtprovider', "kvm"]
      end

      node.vm.provision "shell" do |s|
          s.inline = basic
          s.args = [node_name, node_addr, cluster_ip_nodes]
      end

      # mount the host directories
      node.vm.synced_folder "./", "/import", rsync: true

      # populate /etc/hosts entries
      num_nodes.times do |node_id|
        node.vm.provision "shell" do |s|
          s.inline = "echo '#{node_ips[node_id]} #{node_names[node_id]}' >> /etc/hosts"
        end
      end

      # port mappings
      if n == 0 then
          node.vm.network "forwarded_port", guest: 80, host: 9980, auto_correct: true
      end

      # Init a swarm cluster
      if n == 0 then
          node.vm.provision "shell", inline: <<-SHELL
              # init swarm manager
              sudo docker swarm init --advertise-addr #{node_addr}
          SHELL
      else
          node.vm.provision "shell", inline: <<-SHELL
              # start swarm worker
          SHELL
      end
    end
  end
end
